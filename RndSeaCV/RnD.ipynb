{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_folder():\n",
    "    folder_name = 'scraped_data'\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    return folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_to_csv(data, folder_name):\n",
    "    csv_file_path = os.path.join(folder_name, 'scraped_data.csv')\n",
    "    data.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Scraped data saved to: {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news(url, text_selector='p'):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        article_text = ' '.join([element.text for element in soup.select(text_selector)])\n",
    "\n",
    "        return article_text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_in_section(base_url, section, max_articles=10):\n",
    "    url = f'{base_url}/{section}'\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        article_urls = [a['href'] for a in soup.select('a.qa-heading-link')]\n",
    "\n",
    "        articles = []\n",
    "        for article_url in article_urls[:max_articles]:\n",
    "            article_text = scrape_news(f'{base_url}{article_url}')\n",
    "            if article_text:\n",
    "                articles.append({'text': article_text, 'section': section})\n",
    "        return articles\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(X_train, y_train, X_test, y_test, folder_name):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    param_dist = {'alpha': [0.1, 0.5, 1.0, 2.0]}\n",
    "    random_search = RandomizedSearchCV(MultinomialNB(), param_distributions=param_dist, n_iter=4, cv=5, scoring='accuracy', random_state=42)\n",
    "    random_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    best_alpha = random_search.best_params_['alpha']\n",
    "    print(f\"Best alpha: {best_alpha}\")\n",
    "\n",
    "    model = MultinomialNB(alpha=best_alpha)\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    model_file_path = os.path.join(folder_name, 'text_classification_model.joblib')\n",
    "    joblib.dump(model, model_file_path)\n",
    "\n",
    "    predictions = model.predict(X_test_tfidf)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    report = classification_report(y_test, predictions)\n",
    "    confusion_mat = confusion_matrix(y_test, predictions)\n",
    "\n",
    "    results = pd.DataFrame({'Accuracy': [accuracy]})\n",
    "    results.to_csv(os.path.join(folder_name, 'classification_results.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    base_url = 'https://www.bbc.com'\n",
    "    sections = ['news/world', 'news/uk', 'news/business', 'news/technology', 'news/science_and_environment', 'news/health', 'news/education', 'news/entertainment_and_arts']\n",
    "\n",
    "    data = {'text': [], 'section': []}\n",
    "\n",
    "    for section in sections:\n",
    "        articles = scrape_articles_in_section(base_url, section)\n",
    "        data['text'].extend(article['text'] for article in articles)\n",
    "        data['section'].extend(article['section'] for article in articles)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    folder_name = create_data_folder()\n",
    "    save_data_to_csv(df, folder_name)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['text'], df['section'], test_size=0.2, random_state=42)\n",
    "\n",
    "    train_and_evaluate_model(X_train, y_train, X_test, y_test, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to: scraped_data\\scraped_data.csv\n",
      "Best alpha: 0.1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
